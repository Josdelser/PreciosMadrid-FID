---
title: "Pricing"
output:
  pdf_document: default
  html_document: default
date: "2023-01-04"
---

Realizado por : Jose Delgado Serrano (Grupo 1)
GitHub: https://github.com/Josdelser/PreciosMadrid-FID
Kaggle: https://www.kaggle.com/datasets/mapecode/madrid-province-rent-data


Paquetes y librerias
```{r}
install.packages("tidyverse")
install.packages("dplyr")
install.packages("rattle")
# libraries
library(rpart)
library(rpart.plot)
library(rattle)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(caret)
library(ggfortify)
library(readr)
library(factoextra)


require(corrplot)
set.seed(1)
```




Lectura del dataset 
```{r}
precios_madrid <- read.csv("PreciosMadrid.csv")
head(precios_madrid)
colnames(precios_madrid)


```
Despues de analizar el dataset elijo las columnas que parecen mas interesantes
```{r}
predata1 <- select(precios_madrid,price,floor_built,bathrooms,terrace,bedrooms,postalcode,garage_included)
head(predata1)
colnames(predata1)

```
Elimino pisos que cuesten 0, tengan 0 habitaciones,esten repetidos o sean NA. Tambien para acotar el dataset vamos a coger solo los anuncios de 10 postalcode
```{r}
predata2 = subset(predata1, price>0 & bedrooms>1 & postalcode>=28001 & postalcode<=28011)
predata2 <- na.omit(predata2)
predata <- unique(predata2)


```

Convertir las columnas de valores char("True","False") en num(1,0).Tambien estandarizo todo en numeric
```{r}
predata$terrace <- as.numeric(as.logical(predata$terrace))
predata$garage_included <- as.numeric(as.logical(predata$garage_included))
predata$postalcode <- as.numeric(as.integer(predata$postalcode))
predata$price <- as.numeric(as.integer(predata$price))
predata$floor_built <- as.numeric(as.integer(predata$floor_built))
predata$bathrooms <- as.numeric(as.integer(predata$bathrooms))
predata$bedrooms <- as.numeric(as.integer(predata$bedrooms))

```

Finalmente despues del preprocesamiento de datos, obtenemos el dataset final
```{r}
data<-predata

```


Una vez con los datos bien definido, pasamos a la visualización.

Primero vamos a ver el número de casas por código postal. 
```{r}
barplot(table(data$postalcode),
main="Número de casas por zona(Código Postal)",
xlab="Código Postal",
ylab="Número de casas",)
```

Vamos a etiquetar y categorizar según los metros construidos, con el objetivo de ver los tipos de viviendas
```{r}
data_metros <- data

head(data_metros[order(data_metros$floor_built),])
rangos <- c(0,65,100,150,Inf)
values <- c ('Piso','Duplex','Casa','Chalet')

data_metros$tipo <- cut(data_metros$floor_built, breaks = rangos, labels = values)

barplot(table(data_metros$tipo),
main="Número de casas según el tipo",
xlab="Tipo",
ylab="Número de casas",
col=c("red","orange","green","blue"),) 
```




Vamos a calcular el precio medio, metros construidos y los tipos de casas dependiendo de la zona (Código postal). Para el tipo de casa pasamos a numeric con la asignación por defecto el 1 a pisos, 2 a duplex... De esta manera obtendremos una media del tipo de casa por zona
```{r}
data_metros$tipo <- as.numeric(as.factor(data_metros$tipo))
media1 <- aggregate(data_metros[, c(1,2,8)], list(data_metros$postalcode), mean)
media1

```

Vamos a graficar la media de precio según los metros construidos de media en barras, para ver si vemos algo interesante

```{r}
ggplot(media1, aes(x = floor_built, y = price)) +
        geom_col()
```
Nos percatamos de que en algunos casos en los que no se cumple que a mayor metros mayor precio.Vamos a graficar la media de precio según los metros construidos de media por zonas en puntos
```{r}
#Pasamos a factor el codigo postal para ver mejor el codigo de colores
media1$Group.1 <- as.factor(as.numeric(media1$Group.1))
ggplot(media1, aes(x= floor_built, y=price, colour=Group.1)) + geom_point() 
```
También vamos a hacer algunos analisis para ver como influyen otras variables, como el garaje, terraza...

Gráfica para saber dependiendo del tipo de casa si lleva garage o no. Podriamos hacer esto con las diferentes variables, solo habría que cambiar el valor del aes.

```{r}
a<- ggplot(data_metros, aes(garage_included)) 
a + geom_bar(aes(fill = tipo))

```


Gráfica según el número de tipos de casas por zona
```{r}
a<- ggplot(data_metros, aes(postalcode)) 

a + geom_bar(aes(fill = tipo))
```

Analsis supervisado:


Ahora vamos a realizar un arbol de decisión sobre la variable precio.

Primero partimos el dataset para tener datos de entrenamiento y datos de validacion.
```{r}
train <-createDataPartition(data$price, p = 0.7, list=FALSE)
data_train <- data[train,]
data_val <- data[-train,]
nrow(data_train)
nrow(data_val)


```


Realizando el arbol podemos ver las diferentes variables que afectan a su precio y en que nos podemos basar para aproximar el precio medio de una vivienda.
```{r}
arbol <- rpart(formula =  price  ~ ., data = data_train)
fancyRpartPlot(arbol)
```
Hacemos un par de cambios en la visualizacion para verlo mejor.
```{r}
prp(arbol, type = 2, nn = TRUE, 
    fallen.leaves = FALSE,
    varlen = 0,  shadow.col = "gray")
```
Observamos que los metros construidos y los cuartos de baños son las variables que mas afectan.



Vamos a ver el error relativo y el cp
```{r}
arbol$cptable
plotcp(arbol)

```

Podamos el arból para reducirlo, usando el cp obtenido anteriormente
```{r}
arbol_podado <- prune(arbol, cp = 0.052)
prp(arbol_podado, type = 2, nn = TRUE, 
    fallen.leaves = FALSE,
    varlen = 0)

```
Predecimos en el data de validacion
```{r}
precio_pred <- predict(arbol, newdata = data_val)


precio_pred[1]
data_val[1,1]
```

El valor predeccido no difiere mucho del valor real como podemos ver.


Vamos a hacer predicciones con datos nuevo del precio según el arbol, para ello vamos a crear valores de pruebas.

```{r}
price <- c(0,0,0,0,0)
floor_built<- c(134,134,234,123,100)
bathrooms<- c(1,3,1,2,3)
terrace<- c(0,0,1,0,0)
bedrooms<- c(2,2,4,2,2)
postalcode<- c(28002,28002,28003,28003,28008)
garage_included<- c(1,1,0,0,1)

data_test_nuevo <- data.frame(price,floor_built,bathrooms,terrace,bedrooms,postalcode,garage_included)


#Predeccimos el precio segun el arbol
nuevo_precio_pred <- predict(arbol, newdata = data_test_nuevo)
 
 
nuevo_precio_pred


```

Ahora pasamos a aplicar un metodo de regresion multiple para calcular el precio en funcion de las demas variables

```{r}

regresion_mul <- lm(formula = price ~ ., data = data_train)

summary(regresion_mul)


```
Segun el R^2 el modelo puede explicar en un 59% la variabilidad del precio, ya que R^2=0,5982. Tambien vemos que alguna variable predictor está relacionada con el precio ya que el p-value es bastante infimo.. No encontramos ninguna variable con un p-value alto por lo que nos indican que todas contribuyen en parte al modelo. 





El metodo step, nos arroja que los metros construidos y los cuarto de baños son las variables que mas correlacion tienen. Los
```{r}
step(regresion_mul, direction = "both", trace = 0)
```



Hacemos un grafico corrplot para ver la correlacion con las variables, donde sacamos que las variables que mas influyen son los metros construidos, los cuarto de baños y las camas. Vemos según diferentes métodos que esas son las variables con mayor correlacion.
```{r}

corrplot(cor(data_train) ,type = "lower",)
```


Ahora pasamos a graficar los residuos en funcion de los valores ajustados, es decir distancias entre los estimados y reales.

```{r}

plot(regresion_mul$fitted.values,  regresion_mul$residuals,
     xlab = "Fitted values", ylab = "Residuals")


```
Sacamos el grafico Q-Q para comprar los residuos de dos distribuciones de probabilidad cuando trazamos los cuantiles entre ellos. No apreciamos ningun patron y podemos intuir que esta formanod una linea por lo que concluimos con que el modelo es bueno.
```{r}
qqnorm(regresion_mul$residuals, ylab = "Residual Quantiles")
qqline(regresion_mul$residuals, col = 2)
```


Y vamos a predecir según la regresion multiple, luego compararemos con el arbol de decision.
```{r}


data_val_regresion <-predict(regresion_mul, data_val)
predict(regresion_mul,data_test_nuevo)

data_val_regresion[1]
data_val[1,1]
```
Primero vemos los datos arrojados por la prediccion en los valores de pruebas creados anteriormente,  no distan muchos del de los aboles. Luego, al igual que con los arboles, comparamos el valor del precio real con el calculado con la regresion, vemos que está mas lejano que los arboles y por tanto ha hecho peor prediccion









No supervisado: Clustering con k-means



Primero pasamos a ver que las variables sean numericas

```{r}
lapply(data,class)
```

Comprobamos si necesitamos escalarlas. 
```{r}
summary(data)
```

Vemos que hay mucha diferencia entre el maximo y el minimo por lo cual sería necesario escalarlo, para tener una mejor vision global
```{r}
data_scaled = scale(data)
summary(data_scaled)
```


Una vez lo datos bien estructurados, pasamos a calcular el mejor valor de k. Para ello iremos iterando el valor de 1 a 20 para guardar el valor de compatacion en cada iteracion, luego lo graficaremos y veremos a el k que produce un cambio considerable. Inicializamos la variables nstart a 10 para controlar la aleatoridad.


```{r}

v_compac <-0

for(i in 1:20){
  
  km1<-kmeans(data,center=i,nstar=10)
  v_compac[i] <- km1$tot.withinss
  
}

par(mfrow = c(1,1))

plot(1:20, v_compac, type = "b", 
     xlab = "Numero de clusters", 
     ylab = "Compactacion")

```

Observamos que en k=3 y k=4 empieza a ver un cambio significativo, por lo cual escogemos estos valores para K. Pasamos a ver los resultados y comparamos

K=3

```{r}
kmeans3 <- kmeans(data, center =3,nstart= 10)
kmeans3
```


Ahora pasamos a graficarlo para ver los resultados

```{r}
fviz_cluster(object = kmeans3, data = data, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```

Tenemos muchos valores, por lo que vamos a realizar otro grafico para verlo mas claro.

```{r}
autoplot(kmeans3, data, frame=TRUE)
```




K=4

```{r}
kmeans4 <- kmeans(data, center =4,nstart= 10)
kmeans4
```


Ahora pasamos a graficarlo para ver los resultados, como hemos hecho anteriormente

```{r}
fviz_cluster(object = kmeans4, data = data, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```


```{r}
autoplot(kmeans4, data, frame=TRUE)
```


Obsevarmos que con K=4 tenemos todos los cluster con un gran numero de valores aunque al graficarlos vemos que están muy juntos por la zona media.