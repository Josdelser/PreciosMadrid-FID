---
title: "Pricing"
output:
  pdf_document: default
  html_document: default
date: "2023-01-04"
---
Paquetes y librerias
```{r}

# libraries
library(rpart)
library(rpart.plot)
library(rattle)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(caret)
library(ggfortify)
library(readr)
library(factoextra)


require(corrplot)
set.seed(1)
```




Lectura del dataset 
```{r}
precios_madrid <- read.csv("PreciosMadrid.csv")
head(precios_madrid)
colnames(precios_madrid)


```
Elijo las columnas que parecen mas interesantes
```{r}
predata1 <- select(precios_madrid,price,floor_built,bathrooms,terrace,bedrooms,postalcode,garage_included)
head(predata1)
colnames(predata1)

```
Elimino pisos que cuesten 0, tengan 0 habitaciones,esten repetidos o sean NA. Tambien para acotar el dataset vamos a coger solo los anuncios de 10 postalcode
```{r}
predata2 = subset(predata1, price>0 & bedrooms>1 & postalcode>=28001 & postalcode<=28011)
predata2 <- na.omit(predata2)
predata <- unique(predata2)


```

Convertir las columnas de valores char("True","False") en num(1,0).Tambien estandarizo todo en numeric
```{r}
predata$terrace <- as.numeric(as.logical(predata$terrace))
predata$garage_included <- as.numeric(as.logical(predata$garage_included))
predata$postalcode <- as.numeric(as.integer(predata$postalcode))
predata$price <- as.numeric(as.integer(predata$price))
predata$floor_built <- as.numeric(as.integer(predata$floor_built))
predata$bathrooms <- as.numeric(as.integer(predata$bathrooms))
predata$bedrooms <- as.numeric(as.integer(predata$bedrooms))

```

Finalmente despues del preprocesamiento de datos, obtenemos el dataset final
```{r}
data<-predata

```


Una vez con los datos bien definido, pasamos a la visualización.

Primero vamos a ver el número de casas por código postal. 
```{r}
barplot(table(data$postalcode),
main="Número de casas por zona(Código Postal)",
xlab="Código Postal",
ylab="Número de casas",)
```

Vamos a etiquetar y categorizar según los metros construidos.
```{r}
data_metros <- data

head(data_metros[order(data_metros$floor_built),])
rangos <- c(0,65,100,150,Inf)
values <- c ('Piso','Duplex','Casa','Chalet')

data_metros$tipo <- cut(data_metros$floor_built, breaks = rangos, labels = values)

barplot(table(data_metros$tipo),
main="Número de casas según el tipo",
xlab="Tipo",
ylab="Número de casas",
col=c("red","orange","green","blue"),) 
```




Vamos a calcular el precio medio y metros construidos dependiendo de la zona donde vivas (Código postal)
```{r}

media1 <- aggregate(data[, 1:2], list(data$postalcode), mean)
media1

```

Vamos a graficar la media de precio según los metros construidos de media en  en barras

```{r}
ggplot(media1, aes(x = floor_built, y = price)) +
        geom_col()
```
Vamos a graficar la media de precio según los metros construidos de media por zonas en puntos
```{r}
#Pasamos a factor el codigo postal para ver mejor el codigo de colores
media1$Group.1 <- as.factor(as.numeric(media1$Group.1))
ggplot(media1, aes(x= floor_built, y=price, colour=Group.1)) + geom_point() 
```

Gráfica para saber dependiendo del tipo de casas si lleva garage o no. Podriamos hacer esto con las diferentes variables, solo habría que cambiar el valor del aes.

```{r}
a<- ggplot(data_metros, aes(garage_included)) 
a + geom_bar(aes(fill = tipo))

```


Gráfica según el número de tipos de casas por zona
```{r}
a<- ggplot(data_metros, aes(postalcode)) 

a + geom_bar(aes(fill = tipo))
```

Ahora vamos a realizar un arbol de decision sobre la variable precio.

Primero partimos el dataset para tener datos de entrenamiento y datos de validacion.
```{r}
train <-createDataPartition(data$price, p = 0.7, list=FALSE)
data_train <- data[train,]
data_val <- data[-train,]
nrow(data_train)
nrow(data_val)


```


Podemos ver las diferentes variables que afectan a su precio y en que nos podemos basar para aproximar el precio medio de una vivienda.
```{r}
arbol <- rpart(formula =  price  ~ ., data = data_train)
fancyRpartPlot(arbol)
```
Hacemos un par de cambios en la visualizacion para verlo mejor.
```{r}
prp(arbol, type = 2, nn = TRUE, 
    fallen.leaves = FALSE,
    varlen = 0,  shadow.col = "gray")
```
Vamos a ver el error relativo
```{r}
arbol$cptable
plotcp(arbol)

```

Podamos el arból para reducirlo
```{r}
arbol_podado <- prune(arbol, cp = 0.052)
prp(arbol_podado, type = 2, nn = TRUE, 
    fallen.leaves = FALSE,
    varlen = 0)

```
Predecimos en el data de validacion y vemos que nos arroja los datos correctos
```{r}
precio_pred <- predict(arbol, newdata = data_val)


precio_pred[1]
```

Vamos a hacer predicciones con datos nuevo del precio según el arbol, para ello vamos a crear valores de pruebas.

```{r}
price <- c(0,0,0,0,0)
floor_built<- c(134,134,234,123,100)
bathrooms<- c(1,3,1,2,3)
terrace<- c(0,0,1,0,0)
bedrooms<- c(2,2,4,2,2)
postalcode<- c(28002,28002,28003,28003,28008)
garage_included<- c(1,1,0,0,1)

data_test_nuevo <- data.frame(price,floor_built,bathrooms,terrace,bedrooms,postalcode,garage_included)

#Construimos unos de pruebas y ponemos a 0 la columna del precio. 

#Predeccimos el precio segun el arbol
 nuevo_precio_pred <- predict(arbol, newdata = data_test_nuevo)
 
 
nuevo_precio_pred


```

Ahora pasamos a aplicar un metodo de regresion multiple para calcular el precio en funcion de las demas variables

```{r}

regresion_mul <- lm(formula = price ~ ., data = data)

summary(regresion_mul)


```
Segun el R^2 el modelo puede explicar en un 60% la variabilidad del precio, ya que R^2=0,604. Tambien vemos que alguna variable predictor está relacionada con el precio ya que el p-value es bastante infimo.. No encontramos ninguna variable con un p-value alto por lo que nos indican que todas contribuyen en parte al modelo. 





El metodo step, nos arroja que los metros construidos, los cuarto de baños y el codigo postal son las variables que mas correlacion tienen
```{r}
step(regresion_mul, direction = "both", trace = 0)
```



Hacemos un grafico corrplot para ver la correlacion con las variables, donde sacamos que las variables que mas influyen son los metros construidos, los cuarto de baños y el codigo postal. Vemos según diferentes métodos que esas son las variables con mayor correlacion.
```{r}

corrplot(cor(data) ,type = "lower",)
```


Ahora pasamos a graficar los residuos en funcion de los valores ajustados, es decir distancias entre los estimados y reales.

```{r}

plot(regresion_mul$fitted.values,  regresion_mul$residuals,
     xlab = "Fitted values", ylab = "Residuals")


```
Sacamos el grafico Q-Q para comprar los residuos de dos distribuciones de probabilidad cuando trazamos los cuantiles entre ellos. No apreciamos ningun patron y podemos intuir que esta formanod una linea por lo que concluimos con que el modelo es bueno.
```{r}
qqnorm(regresion_mul$residuals, ylab = "Residual Quantiles")
qqline(regresion_mul$residuals, col = 2)
```


Y vamos a predecir según la regresion multiple, luego compararemos con el arbol de decision
```{r}
predict(regresion_mul,data_test_nuevo)
```










No supervisado: Clustering con k-means



Primero pasamos a ver que las variables sean numericas

```{r}
lapply(data,class)
```

Comprobamos si necesitamos escalarlas. 
```{r}
summary(data)
```

Vemos que hay mucha diferencia entre el maximo y el minimo por lo cual sería necesario escalarlo
```{r}
data_scaled = scale(data)
summary(data_scaled)
```


Una vez lo datos bien estructurados, pasamos a calcular el mejor valor de k. Para ello iremos iterando el valor de 1 a 20 para guardar el valor de compatacion en cada iteracion, luego lo graficaremos y veremos a el k que produce un cambio considerable. Inicializamos la variables nstart a 10 para controlar la aleatoridad.


```{r}

v_compac <-0

for(i in 1:20){
  
  km1<-kmeans(data,center=i,nstar=10)
  v_compac[i] <- km1$tot.withinss
  
}

par(mfrow = c(1,1))

plot(1:20, v_compac, type = "b", 
     xlab = "Numero de clusters", 
     ylab = "Compactacion")

```

Observamos que en k=3 y k=4 empieza a ver un cambio significativo, por lo cual escogemos estos valorres para K. Pasamos a ver los resultados y comparamos

K=3

```{r}
kmeans3 <- kmeans(data, center =3,nstart= 10)
kmeans3
```


Ahora pasamos a graficarlo para ver los resultados

```{r}
fviz_cluster(object = kmeans3, data = data, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```

Tenemos muchos valores, por lo que vamos a realizar otro grafico para verlo mas claro.

```{r}
autoplot(kmeans3, data, frame=TRUE)
```




K=4

```{r}
kmeans4 <- kmeans(data, center =4,nstart= 10)
kmeans4
```


Ahora pasamos a graficarlo para ver los resultados

```{r}
fviz_cluster(object = kmeans4, data = data, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```

Tenemos muchos valores, por lo que vamos a realizar otro grafico para verlo mas claro.

```{r}
autoplot(kmeans4, data, frame=TRUE)
```
