---
title: "Pricing"
output:
  pdf_document: default
  html_document: default
date: "2023-01-04"
---

Realizado por : Jose Delgado Serrano (Grupo 1)
GitHub: https://github.com/Josdelser/PreciosMadrid-FID
Kaggle: https://www.kaggle.com/datasets/mapecode/madrid-province-rent-data


Paquetes y librerias
```{r}
#install.packages("tidyverse")
#install.packages("dplyr")
#install.packages("rattle")
# libraries
library(rpart)
library(rpart.plot)
library(rattle)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(caret)
library(ggfortify)
library(readr)
library(factoextra)


require(corrplot)
set.seed(1)
```




Lectura del dataset 
```{r}
precios_madrid <- read.csv("PreciosMadrid.csv")
head(precios_madrid)
colnames(precios_madrid)


```
Despues de analizar el dataset elijo las columnas que parecen mas interesantes
```{r}
predata1 <- select(precios_madrid,price,floor_built,bathrooms,terrace,bedrooms,postalcode,garage_included)
head(predata1)
colnames(predata1)

```
Elimino pisos que cuesten 0, tengan 0 habitaciones,esten repetidos o sean NA. Tambien para acotar el dataset vamos a coger solo los anuncios de 10 postalcode
```{r}
predata2 = subset(predata1, price>0 & bedrooms>1 & postalcode>=28001 & postalcode<=28011)
predata2 <- na.omit(predata2)
predata <- unique(predata2)


```

Convertir las columnas de valores char("True","False") en num(1,0).Tambien estandarizo todo en numeric
```{r}
predata$terrace <- as.numeric(as.logical(predata$terrace))
predata$garage_included <- as.numeric(as.logical(predata$garage_included))
predata$postalcode <- as.numeric(as.integer(predata$postalcode))
predata$price <- as.numeric(as.integer(predata$price))
predata$floor_built <- as.numeric(as.integer(predata$floor_built))
predata$bathrooms <- as.numeric(as.integer(predata$bathrooms))
predata$bedrooms <- as.numeric(as.integer(predata$bedrooms))

```

Finalmente despues del preprocesamiento de datos, obtenemos el dataset final
```{r}
data<-predata

```


Una vez con los datos bien definido, pasamos a la visualización.


```{r}
barplot(table(data$terrace),
main="Número de casas con terraza",
xlab="Tiene o no terraza",
ylab="Número de casas",)

```

```{r}
barplot(table(data$bathrooms),
main="Cantidad de baños por casa",
xlab="Cantidad de baños",
ylab="Número de casas",)
```

```{r}
barplot(table(data$bedrooms),
main="Cantidad de habitaciones por casa",
xlab="Número de habitaciones",
ylab="Número de casas",)
```



Primero vamos a ver el número de casas por código postal. 
```{r}
barplot(table(data$postalcode),
main="Número de casas por zona(Código Postal)",
xlab="Código Postal",
ylab="Número de casas",)
```

Vamos a etiquetar y categorizar según los metros construidos, con el objetivo de ver los tipos de viviendas
```{r}
data_metros <- data

head(data_metros[order(data_metros$floor_built),])
rangos <- c(0,65,100,150,Inf)
values <- c ('Piso','Duplex','Casa','Chalet')

data_metros$tipo <- cut(data_metros$floor_built, breaks = rangos, labels = values)

barplot(table(data_metros$tipo),
main="Número de casas según el tipo",
xlab="Tipo",
ylab="Número de casas",
col=c("red","orange","green","blue"),) 
```




Vamos a calcular el precio medio, metros construidos y los tipos de casas dependiendo de la zona (Código postal). Para el tipo de casa pasamos a numeric con la asignación por defecto el 1 a pisos, 2 a duplex... De esta manera obtendremos una media del tipo de casa por zona
```{r}
data_metros$tipo <- as.numeric(as.factor(data_metros$tipo))
media1 <- aggregate(data_metros[, c(1,2,8)], list(data_metros$postalcode), mean)
media1

```

Vamos a graficar la media de precio según los metros construidos de media en barras, para ver si vemos algo interesante

```{r}
ggplot(media1, aes(x = floor_built, y = price)) +
        geom_col()
```
Nos percatamos de que en algunos casos en los que no se cumple que a mayor metros mayor precio.Vamos a graficar la media de precio según los metros construidos de media por zonas en puntos
```{r}
#Pasamos a factor el codigo postal para ver mejor el codigo de colores
media1$Group.1 <- as.factor(as.numeric(media1$Group.1))
ggplot(media1, aes(x= floor_built, y=price, colour=Group.1)) + geom_point() 
```
También vamos a hacer algunos analisis para ver como influyen otras variables, como el garaje, terraza...


Gráficas de dispersión, para observar la relación entre las variables. 

```{r}
plot(x = data$price, y = data$floor_built)
plot(x = data$price, y = data$bathrooms)
plot(x = data$price, y = data$postalcode)
plot(x = data$price, y = data$bedrooms)

```


Gráfica para saber dependiendo del tipo de casa si lleva garage o no. Podriamos hacer esto con las diferentes variables, solo habría que cambiar el valor del aes.

```{r}
a<- ggplot(data_metros, aes(garage_included)) 
a + geom_bar(aes(fill = tipo))

```


Gráfica según el número de tipos de casas por zona
```{r}
a<- ggplot(data_metros, aes(postalcode)) 

a + geom_bar(aes(fill = tipo))
```

Analasis supervisado:

Después de haber estado jugando con las variables, vamos a aplicar diferentes tecnicas. Primero categirizamos los precios del dataset

```{r}
data_analisis<-data
nrow(data_analisis)
```


```{r}

rangos <- c(0,1300,2500,Inf)
values <- c ('barata','normal','cara')

data_analisis$categoria <- cut(data_analisis$price, breaks = rangos, labels = values)
```

```{r}

data_analisis <- data_analisis %>%
select(-c(price))
```


```{r}
data_agrupada <- data_analisis %>%
group_by( floor_built, bathrooms, bedrooms, garage_included, terrace,postalcode,categoria) %>%
count() %>%
arrange(desc(n()))

```

```{r}
data_agrupada <- data_agrupada  %>%
rename(TotalCasas = n)
head(data_agrupada)
```

Primero partimos el dataset para tener datos de entrenamiento y datos de validacion.
```{r}
train <-createDataPartition(data_agrupada$categoria, p = 0.7, list=FALSE)
validos <-createDataPartition(data_agrupada$categoria, p = 0.7, list=FALSE)
data_train <- data_agrupada[train,]
data_val <- data_agrupada[validos,]
nrow(data_train)
nrow(data_val)


```

Realizando el arbol podemos ver las diferentes variables que afectan a su precio y en que nos podemos basar para aproximar el precio medio de una vivienda.
```{r}
arbol <- rpart(formula =  categoria  ~ ., data = data_train, method = 'class')

fancyRpartPlot(arbol)

```


Hacemos un par de cambios en la visualizacion para verlo mejor.
```{r}
rpart.plot(arbol, extra = 5)
prp(arbol, type = 2, nn = TRUE, 
    fallen.leaves = FALSE,
    varlen = 0,  shadow.col = "gray")
```
Observamos que los metros construidos y los cuartos de baños son las variables que mas afectan.



Vamos a ver el cp
```{r}
arbol$cptable
plotcp(arbol)

```

Podamos el arból para reducirlo, usando el cp obtenido anteriormente
```{r}
arbol_podado <- prune(arbol, cp = 0.017)
prp(arbol_podado, type = 2, nn = TRUE, 
    fallen.leaves = FALSE,
    varlen = 0)

```
Predecimos en el data de validacion y sacamos la matrix de confusion

```{r}
categoria_pred <- predict(arbol, newdata = data_val, type="class")

confunsion_m <- table(data_val$categoria, categoria_pred)
confunsion_m

```
Calculamos el acierto

```{r}
acierto <- sum(diag(confunsion_m)) / sum(confunsion_m)
acierto
```
Vemos que solo tenemos un 67% de acierto, por lo que vamos a controlar el algoritmo de particion para ver si mejoramos

```{r}
controlador <- rpart.control(minsplit = 10, minbucket = round(10 / 3), maxdepth = 4, cp = 0,015)
```

Luego volvemos a repetir todo el proceso
```{r}
arbol2 <- rpart(categoria ~., data = data_train, method = 'class', control = control)

categoria_pred1 <- predict(arbol2, newdata = data_val, type="class")
confunsion_m1 <- table(data_val$categoria, categoria_pred1)
acierto2 <- sum(diag(confunsion_m1)) / sum(confunsion_m1)
acierto2
```
Vemos que no mejoramos, por lo cual con la primera iteracion hemos sacado un % optimo.




Ahora pasamos a aplicar un metodo de regresion multiple para calcular el precio en funcion de las demas variables. Para esto si vamos a necesitar el precio y no la categoria

```{r}

train_rg <-createDataPartition(data$price, p = 0.7, list=FALSE)
data_train_rg <- data[train_rg,]
data_val_rg <- data[-train_rg,]
nrow(data_train_rg)
nrow(data_val_rg)

```

```{r}

regresion_mul <- lm(formula = price ~ ., data = data_train_rg)

summary(regresion_mul)


```
Segun el R^2 el modelo puede explicar en un 61% la variabilidad del precio, ya que R^2=0,6132 Tambien vemos que alguna variable predictor está relacionada con el precio ya que el p-value es bastante infimo.. No encontramos ninguna variable con un p-value alto por lo que nos indican que todas contribuyen en parte al modelo. 





El metodo step, nos arroja que los metros construidos y los cuarto de baños son las variables que mas correlacion tienen. Los
```{r}
step(regresion_mul, direction = "both", trace = 0)
```



Hacemos un grafico corrplot para ver la correlacion con las variables, donde sacamos que las variables que mas influyen son los metros construidos, los cuarto de baños y las camas. Vemos según diferentes métodos que esas son las variables con mayor correlacion.
```{r}

corrplot(cor(data_train_rg) ,type = "lower",)
```


Ahora pasamos a graficar los residuos en funcion de los valores ajustados, es decir distancias entre los estimados y reales.

```{r}

plot(regresion_mul$fitted.values,  regresion_mul$residuals,
     xlab = "Fitted values", ylab = "Residuals")


```
Sacamos el grafico Q-Q para comprar los residuos de dos distribuciones de probabilidad cuando trazamos los cuantiles entre ellos. No apreciamos ningun patron y podemos intuir que esta formanod una linea por lo que concluimos con que el modelo es bueno.
```{r}
qqnorm(regresion_mul$residuals, ylab = "Residual Quantiles")
qqline(regresion_mul$residuals, col = 2)
```


Y vamos a predecir según la regresion multiple, luego compararemos con el arbol de decision.
```{r}


data_val_regresion <-predict(regresion_mul, data_val_rg)
data_val_regresion
```
Vamos a calcular el RMSE.Primero hacemos la diferencias entre los valores reales y los predecidos, luego el residual de la suma y el valor del acierto
```{r}
dif <- data_val_rg$price-data_val_regresion
rmse <- sqrt(mean(dif ^ 2))
rmse
```
Vemos que el RSME es una diferencia bastante grande. Pasamos a calcular el acierto
```{r}
residual <- sum(dif^2)
total <- sum((data_val_rg$price - mean(data_val_rg$price)) ^ 2)
acierto_rg <- 1 - (residual / total)
acierto_rg
```






No supervisado: Clustering con k-means



Primero pasamos a ver que las variables sean numericas

```{r}
lapply(data,class)
```

Comprobamos si necesitamos escalarlas. 
```{r}
summary(data)
```

Vemos que hay mucha diferencia entre el maximo y el minimo por lo cual sería necesario escalarlo, para tener una mejor vision global
```{r}
data_scaled = scale(data)
summary(data_scaled)
```


Una vez lo datos bien estructurados, pasamos a calcular el mejor valor de k. Para ello iremos iterando el valor de 1 a 20 para guardar el valor de compatacion en cada iteracion, luego lo graficaremos y veremos a el k que produce un cambio considerable. Inicializamos la variables nstart a 10 para controlar la aleatoridad.


```{r}

v_compac <-0

for(i in 1:20){
  
  km1<-kmeans(data,center=i,nstar=10)
  v_compac[i] <- km1$tot.withinss
  
}

par(mfrow = c(1,1))

plot(1:20, v_compac, type = "b", 
     xlab = "Numero de clusters", 
     ylab = "Compactacion")

```

Observamos que en k=3 y k=4 empieza a ver un cambio significativo, por lo cual escogemos estos valores para K. Pasamos a ver los resultados y comparamos

K=3

```{r}
kmeans3 <- kmeans(data, center =3,nstart= 10)
kmeans3
```


Ahora pasamos a graficarlo para ver los resultados

```{r}
fviz_cluster(object = kmeans3, data = data, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```

Tenemos muchos valores, por lo que vamos a realizar otro grafico para verlo mas claro.

```{r}
autoplot(kmeans3, data, frame=TRUE)
```




K=4

```{r}
kmeans4 <- kmeans(data, center =4,nstart= 10)
kmeans4
```


Ahora pasamos a graficarlo para ver los resultados, como hemos hecho anteriormente

```{r}
fviz_cluster(object = kmeans4, data = data, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```


```{r}
autoplot(kmeans4, data, frame=TRUE)
```


Obsevarmos que con K=4 tenemos todos los cluster con un gran numero de valores aunque al graficarlos vemos que están muy juntos por la zona media.


Pasamos el Clustering jerarquico

Construimos el dendograma
```{r}

matriz_distancias <-dist(data)
den1 = hclust(matriz_distancias, method = "complete")
summary(den1)
```
Pasamos a visualizarlo
```{r}
plot(den1,hang = -10)
```

Decidimos cortar según el numero de clusters
```{r}
cortado_den1<-cutree(den1,k=4)
```

Y comparamos con k-means


```{r}
table(kmeans4$cluster,cortado_den1)
```




